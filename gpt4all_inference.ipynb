{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quintian/NLP-with-LLMs/blob/main/gpt4all_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT4All CPU Interface\n",
        "\n",
        "In this notebook, we jump as quickly as we can into a command-line interaction with a GPT-4-like model that is on our own device."
      ],
      "metadata": {
        "id": "Powa-rm3ApS0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVpLf3_fX3qK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install nomic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nomic"
      ],
      "metadata": {
        "id": "m27hzyQZqJS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show nomic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NdwgKmrs4Uq",
        "outputId": "957d8e76-6657-420d-c750-cf29a3025e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: nomic\n",
            "Version: 3.4.1\n",
            "Summary: The official Nomic python client.\n",
            "Home-page: https://github.com/nomic-ai/nomic\n",
            "Author: nomic.ai\n",
            "Author-email: support@nomic.ai\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: click, jsonlines, loguru, numpy, pandas, pillow, pyarrow, pydantic, pyjwt, requests, rich, tqdm\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGZfr2S4uR7a",
        "outputId": "ff4c99cc-7297-47ad-95c4-b5ec0d17b3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gpt4all) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gpt4all) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2025.1.31)\n",
            "Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gpt4all\n",
            "Successfully installed gpt4all-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n"
      ],
      "metadata": {
        "id": "2c2RhfhUuaPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all\n",
        "!gpt4all-cli download\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYKGfPEmwel4",
        "outputId": "2c1d88c8-d77a-43fd-8253-e9565468a4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gpt4all in /usr/local/lib/python3.11/dist-packages (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gpt4all) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gpt4all) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2025.1.31)\n",
            "/bin/bash: line 1: gpt4all-cli: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "\n",
        "models = GPT4All.list_models()\n",
        "quantized_models = [model for model in models if \"quantized\" in model['name'].lower()]\n",
        "print(quantized_models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y74N5cLhwzGO",
        "outputId": "621fc820-dc0e-4fc4-9834-d30c67da54b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [f\"Model {i}\" for i in range(50)]  # Example list of models\n",
        "\n",
        "# Print models in chunks of 10\n",
        "for i in range(0, len(models), 10):\n",
        "    print(models[i:i+10])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2wIxB9GNCzD",
        "outputId": "dee5ec9d-5bcd-419f-e200-5e15ebdc32fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Model 0', 'Model 1', 'Model 2', 'Model 3', 'Model 4', 'Model 5', 'Model 6', 'Model 7', 'Model 8', 'Model 9']\n",
            "['Model 10', 'Model 11', 'Model 12', 'Model 13', 'Model 14', 'Model 15', 'Model 16', 'Model 17', 'Model 18', 'Model 19']\n",
            "['Model 20', 'Model 21', 'Model 22', 'Model 23', 'Model 24', 'Model 25', 'Model 26', 'Model 27', 'Model 28', 'Model 29']\n",
            "['Model 30', 'Model 31', 'Model 32', 'Model 33', 'Model 34', 'Model 35', 'Model 36', 'Model 37', 'Model 38', 'Model 39']\n",
            "['Model 40', 'Model 41', 'Model 42', 'Model 43', 'Model 44', 'Model 45', 'Model 46', 'Model 47', 'Model 48', 'Model 49']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = GPT4All.list_models()\n",
        "for i in range(0, len(models), 10):  # Print models in chunks of 10\n",
        "    print(models[i:i+10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7kvO625K_yt",
        "outputId": "f3f5ea65-332e-4645-b201-96768a124e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'order': 'a', 'md5sum': 'a54c08a7b90e4029a8c2ab5b5dc936aa', 'name': 'Reasoner v1', 'filename': 'qwen2.5-coder-7b-instruct-q4_0.gguf', 'filesize': '4431390720', 'requires': '3.6.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Based on <a href=\"https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct\">Qwen2.5-Coder 7B</a></li><li>Uses built-in javascript code interpreter</li><li>Use for complex reasoning tasks that can be aided by computation analysis</li><li>License: <a href=\"https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE\">Apache License Version 2.0</a></li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_0.gguf', 'chatTemplate': \"{{- '<|im_start|>system\\\\n' }}\\n{% if toolList|length > 0 %}You have access to the following functions:\\n{% for tool in toolList %}\\nUse the function '{{tool.function}}' to: '{{tool.description}}'\\n{% if tool.parameters|length > 0 %}\\nparameters:\\n{% for info in tool.parameters %}\\n  {{info.name}}:\\n    type: {{info.type}}\\n    description: {{info.description}}\\n    required: {{info.required}}\\n{% endfor %}\\n{% endif %}\\n# Tool Instructions\\nIf you CHOOSE to call this function ONLY reply with the following format:\\n'{{tool.symbolicFormat}}'\\nHere is an example. If the user says, '{{tool.examplePrompt}}', then you reply\\n'{{tool.exampleCall}}'\\nAfter the result you might reply with, '{{tool.exampleReply}}'\\n{% endfor %}\\nYou MUST include both the start and end tags when you use a function.\\n\\nYou are a helpful AI assistant who uses the functions to break down, analyze, perform, and verify complex reasoning tasks. You SHOULD try to verify your answers using the functions where possible.\\n{% endif %}\\n{{- '<|im_end|>\\\\n' }}\\n{% for message in messages %}\\n{{'<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>\\\\n' }}\\n{% endfor %}\\n{% if add_generation_prompt %}\\n{{ '<|im_start|>assistant\\\\n' }}\\n{% endif %}\\n\", 'systemPrompt': ''}, {'order': 'aa', 'md5sum': 'c87ad09e1e4c8f9c35a5fcef52b6f1c9', 'name': 'Llama 3 8B Instruct', 'filename': 'Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'filesize': '4661724384', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href=\"https://llama.meta.com/llama3/license/\">Meta Llama 3 Community License</a></li></ul>', 'url': 'https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\\n\\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n%2<|eot_id|>', 'systemPrompt': '', 'chatTemplate': \"{%- set loop_messages = messages %}\\n{%- for message in loop_messages %}\\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' %}\\n    {{- content }}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\"}, {'order': 'aa1', 'sha256sum': '5cd4ee65211770f1d99b4f6f4951780b9ef40e29314bd6542bb5bd0ad0bc29d1', 'name': 'DeepSeek-R1-Distill-Qwen-7B', 'filename': 'DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'filesize': '4444121056', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-7B distillation of DeepSeek-R1.</p><ul><li>License: <a href=\"https://opensource.org/license/mit\">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf', 'chatTemplate': \"{%- if not add_generation_prompt is defined %}\\n    {%- set add_generation_prompt = false %}\\n{%- endif %}\\n{%- if messages[0]['role'] == 'system' %}\\n    {{- messages[0]['content'] }}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '<｜User｜>' + message['content'] }}\\n    {%- endif %}\\n    {%- if message['role'] == 'assistant' %}\\n        {%- set content = message['content'] | regex_replace('^[\\\\\\\\s\\\\\\\\S]*</think>', '') %}\\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\\n    {%- endif %}\\n{%- endfor -%}\\n{%- if add_generation_prompt %}\\n    {{- '<｜Assistant｜>' }}\\n{%- endif %}\"}, {'order': 'aa2', 'sha256sum': '906b3382f2680f4ce845459b4a122e904002b075238080307586bcffcde49eef', 'name': 'DeepSeek-R1-Distill-Qwen-14B', 'filename': 'DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'filesize': '8544267680', 'requires': '3.8.0', 'ramrequired': '16', 'parameters': '14 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-14B distillation of DeepSeek-R1.</p><ul><li>License: <a href=\"https://opensource.org/license/mit\">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf', 'chatTemplate': \"{%- if not add_generation_prompt is defined %}\\n    {%- set add_generation_prompt = false %}\\n{%- endif %}\\n{%- if messages[0]['role'] == 'system' %}\\n    {{- messages[0]['content'] }}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '<｜User｜>' + message['content'] }}\\n    {%- endif %}\\n    {%- if message['role'] == 'assistant' %}\\n        {%- set content = message['content'] | regex_replace('^[\\\\\\\\s\\\\\\\\S]*</think>', '') %}\\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\\n    {%- endif %}\\n{%- endfor -%}\\n{%- if add_generation_prompt %}\\n    {{- '<｜Assistant｜>' }}\\n{%- endif %}\"}, {'order': 'aa3', 'sha256sum': '0eb93e436ac8beec18aceb958c120d282cb2cf5451b23185e7be268fe9d375cc', 'name': 'DeepSeek-R1-Distill-Llama-8B', 'filename': 'DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'filesize': '4675894112', 'requires': '3.8.0', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Llama-3.1-8B distillation of DeepSeek-R1.</p><ul><li>License: <a href=\"https://opensource.org/license/mit\">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf', 'chatTemplate': \"{%- if not add_generation_prompt is defined %}\\n    {%- set add_generation_prompt = false %}\\n{%- endif %}\\n{%- if messages[0]['role'] == 'system' %}\\n    {{- messages[0]['content'] }}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '<｜User｜>' + message['content'] }}\\n    {%- endif %}\\n    {%- if message['role'] == 'assistant' %}\\n        {%- set content = message['content'] | regex_replace('^[\\\\\\\\s\\\\\\\\S]*</think>', '') %}\\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\\n    {%- endif %}\\n{%- endfor -%}\\n{%- if add_generation_prompt %}\\n    {{- '<｜Assistant｜>' }}\\n{%- endif %}\"}, {'order': 'aa4', 'sha256sum': 'b3af887d0a015b39fab2395e4faf682c1a81a6a3fd09a43f0d4292f7d94bf4d0', 'name': 'DeepSeek-R1-Distill-Qwen-1.5B', 'filename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'filesize': '1068807776', 'requires': '3.8.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'deepseek', 'description': '<p>The official Qwen2.5-Math-1.5B distillation of DeepSeek-R1.</p><ul><li>License: <a href=\"https://opensource.org/license/mit\">MIT</a></li><li>No restrictions on commercial use</li><li>#reasoning</li></ul>', 'url': 'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf', 'chatTemplate': \"{%- if not add_generation_prompt is defined %}\\n    {%- set add_generation_prompt = false %}\\n{%- endif %}\\n{%- if messages[0]['role'] == 'system' %}\\n    {{- messages[0]['content'] }}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '<｜User｜>' + message['content'] }}\\n    {%- endif %}\\n    {%- if message['role'] == 'assistant' %}\\n        {%- set content = message['content'] | regex_replace('^[\\\\\\\\s\\\\\\\\S]*</think>', '') %}\\n        {{- '<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>' }}\\n    {%- endif %}\\n{%- endfor -%}\\n{%- if add_generation_prompt %}\\n    {{- '<｜Assistant｜>' }}\\n{%- endif %}\"}, {'order': 'b', 'md5sum': '27b44e8ae1817525164ddf4f8dae8af4', 'name': 'Llama 3.2 3B Instruct', 'filename': 'Llama-3.2-3B-Instruct-Q4_0.gguf', 'filesize': '1921909280', 'requires': '3.4.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href=\"https://llama.meta.com/llama3_2/license/\">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\\n\\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\\nCutting Knowledge Date: December 2023\\n\\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': \"{{- bos_token }}\\n{%- set date_string = strftime_now('%d %b %Y') %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] | trim %}\\n    {%- set loop_start = 1 %}\\n{%- else %}\\n    {%- set system_message = '' %}\\n    {%- set loop_start = 0 %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- '<|start_header_id|>system<|end_header_id|>\\\\n\\\\n' }}\\n{{- 'Cutting Knowledge Date: December 2023\\\\n' }}\\n{{- 'Today Date: ' + date_string + '\\\\n\\\\n' }}\\n{{- system_message }}\\n{{- '<|eot_id|>' }}\\n\\n{%- for message in messages %}\\n    {%- if loop.index0 >= loop_start %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n' + message['content'] | trim + '<|eot_id|>' }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\"}, {'order': 'c', 'md5sum': '48ff0243978606fdba19d899b77802fc', 'name': 'Llama 3.2 1B Instruct', 'filename': 'Llama-3.2-1B-Instruct-Q4_0.gguf', 'filesize': '773025920', 'requires': '3.4.0', 'ramrequired': '2', 'parameters': '1 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li>Fast responses</li><li>Instruct model</li><li>Multilingual dialogue use</li><li>Agentic system capable</li><li>Trained by Meta</li><li>License: <a href=\"https://llama.meta.com/llama3_2/license/\">Meta Llama 3.2 Community License</a></li></ul>', 'url': 'https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\\n\\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\\nCutting Knowledge Date: December 2023\\n\\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': \"{{- bos_token }}\\n{%- set date_string = strftime_now('%d %b %Y') %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] | trim %}\\n    {%- set loop_start = 1 %}\\n{%- else %}\\n    {%- set system_message = '' %}\\n    {%- set loop_start = 0 %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- '<|start_header_id|>system<|end_header_id|>\\\\n\\\\n' }}\\n{{- 'Cutting Knowledge Date: December 2023\\\\n' }}\\n{{- 'Today Date: ' + date_string + '\\\\n\\\\n' }}\\n{{- system_message }}\\n{{- '<|eot_id|>' }}\\n\\n{%- for message in messages %}\\n    {%- if loop.index0 >= loop_start %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n' + message['content'] | trim + '<|eot_id|>' }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\"}, {'order': 'd', 'md5sum': 'a5f6b4eabd3992da4d7fb7f020f921eb', 'name': 'Nous Hermes 2 Mistral DPO', 'filename': 'Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'filesize': '4108928000', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Good overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\\n%1<|im_end|>\\n<|im_start|>assistant\\n%2<|im_end|>\\n', 'systemPrompt': '', 'chatTemplate': \"{%- for message in messages %}\\n    {{- '<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>\\\\n' }}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\"}, {'order': 'e', 'md5sum': '97463be739b50525df56d33b26b00852', 'name': 'Mistral Instruct', 'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf', 'filesize': '4108916384', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'systemPrompt': '', 'description': '<strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf', 'promptTemplate': '[INST] %1 [/INST]', 'chatTemplate': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_start = 1 %}\\n{%- else %}\\n    {%- set loop_start = 0 %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if loop.index0 >= loop_start %}\\n        {%- if (message['role'] == 'user') != ((loop.index0 - loop_start) % 2 == 0) %}\\n            {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\\n        {%- endif %}\\n        {%- if message['role'] == 'user' %}\\n            {%- if loop.index0 == loop_start and loop_start == 1 %}\\n                {{- ' [INST] ' + system_message + '\\\\n\\\\n' + message['content'] + ' [/INST]' }}\\n            {%- else %}\\n                {{- ' [INST] ' + message['content'] + ' [/INST]' }}\\n            {%- endif %}\\n        {%- elif message['role'] == 'assistant' %}\\n            {{- ' ' + message['content'] + eos_token }}\\n        {%- else %}\\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\"}]\n",
            "[{'order': 'f', 'md5sum': '8a9c75bcd8a66b7693f158ec96924eeb', 'name': 'Llama 3.1 8B Instruct 128k', 'filename': 'Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'filesize': '4661212096', 'requires': '3.1.1', 'ramrequired': '8', 'parameters': '8 billion', 'quant': 'q4_0', 'type': 'LLaMA3', 'description': '<ul><li><strong>For advanced users only. Not recommended for use on Windows or Linux without selecting CUDA due to speed issues.</strong></li><li>Fast responses</li><li>Chat based model</li><li>Large context size of 128k</li><li>Accepts agentic system prompts in Llama 3.1 format</li><li>Trained by Meta</li><li>License: <a href=\"https://llama.meta.com/llama3_1/license/\">Meta Llama 3.1 Community License</a></li></ul>', 'url': 'https://huggingface.co/GPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k/resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf', 'promptTemplate': '<|start_header_id|>user<|end_header_id|>\\n\\n%1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n%2', 'systemPrompt': '<|start_header_id|>system<|end_header_id|>\\nCutting Knowledge Date: December 2023\\n\\nYou are a helpful assistant.<|eot_id|>', 'chatTemplate': \"{%- set loop_messages = messages %}\\n{%- for message in loop_messages %}\\n    {%- set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' %}\\n    {%- if loop.index0 == 0 %}\\n        {%- set content = bos_token + content %}\\n    {%- endif %}\\n    {{- content }}\\n{%- endfor %}\\n{{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\"}, {'order': 'g', 'md5sum': 'f692417a22405d80573ac10cb0cd6c6a', 'name': 'Mistral OpenOrca', 'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf', 'filesize': '4108928128', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href=\"https://atlas.nomic.ai/\">Nomic Atlas</a><li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\\n%1<|im_end|>\\n<|im_start|>assistant\\n%2<|im_end|>\\n', 'systemPrompt': '<|im_start|>system\\nYou are MistralOrca, a large language model trained by Alignment Lab AI.\\n<|im_end|>\\n', 'chatTemplate': \"{%- for message in messages %}\\n    {{- '<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>\\\\n' }}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\"}, {'order': 'h', 'md5sum': 'c4c78adf744d6a20f05c8751e3961b84', 'name': 'GPT4All Falcon', 'filename': 'gpt4all-falcon-newbpe-q4_0.gguf', 'filesize': '4210994112', 'requires': '2.6.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Falcon', 'systemPrompt': '', 'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf', 'promptTemplate': '### Instruction:\\n%1\\n\\n### Response:\\n', 'chatTemplate': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set loop_start = 1 %}\\n    {{- messages[0]['content'] + '\\\\n\\\\n' }}\\n{%- else %}\\n    {%- set loop_start = 0 %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if loop.index0 >= loop_start %}\\n        {%- if message['role'] == 'user' %}\\n            {{- '### User: ' + message['content'] + '\\\\n\\\\n' }}\\n        {%- elif message['role'] == 'assistant' %}\\n            {{- '### Assistant: ' + message['content'] + '\\\\n\\\\n' }}\\n        {%- else %}\\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '### Assistant:' }}\\n{%- endif %}\"}, {'order': 'i', 'md5sum': '00c8593ba57f5240f59662367b3ed4a5', 'name': 'Orca 2 (Medium)', 'filename': 'orca-2-7b.Q4_0.gguf', 'filesize': '3825824192', 'requires': '2.5.2', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf', 'chatTemplate': \"{%- for message in messages %}\\n    {{- '<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>\\\\n' }}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\"}, {'order': 'j', 'md5sum': '3c0d63c4689b9af7baa82469a6f51a19', 'name': 'Orca 2 (Full)', 'filename': 'orca-2-13b.Q4_0.gguf', 'filesize': '7365856064', 'requires': '2.5.2', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf', 'chatTemplate': \"{%- for message in messages %}\\n    {{- '<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>\\\\n' }}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\"}, {'order': 'k', 'md5sum': '5aff90007499bce5c64b1c0760c0b186', 'name': 'Wizard v1.2', 'filename': 'wizardlm-13b-v1.2.Q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf', 'chatTemplate': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set loop_start = 1 %}\\n    {{- messages[0]['content'] + ' ' }}\\n{%- else %}\\n    {%- set loop_start = 0 %}\\n{%- endif %}\\n{%- for message in loop_messages %}\\n    {%- if loop.index0 >= loop_start %}\\n        {%- if message['role'] == 'user' %}\\n            {{- 'USER: ' + message['content'] }}\\n        {%- elif message['role'] == 'assistant' %}\\n            {{- 'ASSISTANT: ' + message['content'] }}\\n        {%- else %}\\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\\n        {%- endif %}\\n        {%- if (loop.index0 - loop_start) % 2 == 0 %}\\n            {{- ' ' }}\\n        {%- else %}\\n            {{- eos_token }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- 'ASSISTANT:' }}\\n{%- endif %}\", 'systemMessage': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"}, {'order': 'l', 'md5sum': '31b47b4e8c1816b62684ac3ca373f9e1', 'name': 'Ghost 7B v0.9.1', 'filename': 'ghost-7b-v0.9.1-Q4_0.gguf', 'filesize': '4108916960', 'requires': '2.7.1', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.', 'url': 'https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf', 'promptTemplate': '<|user|>\\n%1</s>\\n<|assistant|>\\n%2</s>\\n', 'systemPrompt': '<|system|>\\nYou are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.\\n</s>', 'chatTemplate': \"{%- for message in messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '<|user|>\\\\n' + message['content'] + eos_token }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '<|system|>\\\\n' + message['content'] + eos_token }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- '<|assistant|>\\\\n'  + message['content'] + eos_token }}\\n    {%- endif %}\\n    {%- if loop.last and add_generation_prompt %}\\n        {{- '<|assistant|>' }}\\n    {%- endif %}\\n{%- endfor %}\", 'systemMessage': 'You are Ghost created by Lam Hieu. You are a helpful and knowledgeable assistant. You like to help and always give honest information, in its original language. In communication, you are always respectful, equal and promote positive behavior.'}, {'order': 'm', 'md5sum': '3d12810391d04d1153b692626c0c6e16', 'name': 'Hermes', 'filename': 'nous-hermes-llama2-13b.Q4_0.gguf', 'filesize': '7366062080', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA2', 'systemPrompt': '', 'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf', 'promptTemplate': '### Instruction:\\n%1\\n\\n### Response:\\n', 'chatTemplate': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set loop_start = 1 %}\\n    {{- messages[0]['content'] + '\\\\n\\\\n' }}\\n{%- else %}\\n    {%- set loop_start = 0 %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if loop.index0 >= loop_start %}\\n        {%- if message['role'] == 'user' %}\\n            {{- '### Instruction:\\\\n' + message['content'] + '\\\\n\\\\n' }}\\n        {%- elif message['role'] == 'assistant' %}\\n            {{- '### Response:\\\\n' + message['content'] + '\\\\n\\\\n' }}\\n        {%- else %}\\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '### Instruction:\\\\n' }}\\n{%- endif %}\"}, {'order': 'n', 'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c', 'name': 'Snoozy', 'filename': 'gpt4all-13b-snoozy-q4_0.gguf', 'filesize': '7365834624', 'requires': '2.5.0', 'ramrequired': '16', 'parameters': '13 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf', 'chatTemplate': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set loop_start = 1 %}\\n    {{- messages[0]['content'] + '\\\\n\\\\n' }}\\n{%- else %}\\n    {%- set loop_start = 0 %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if loop.index0 >= loop_start %}\\n        {%- if message['role'] == 'user' %}\\n            {{- '### Instruction:\\\\n' + message['content'] + '\\\\n\\\\n' }}\\n        {%- elif message['role'] == 'assistant' %}\\n            {{- '### Response:\\\\n' + message['content'] + '\\\\n\\\\n' }}\\n        {%- else %}\\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '### Response:\\\\n' }}\\n{%- endif %}\", 'systemMessage': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'}, {'order': 'o', 'md5sum': '15dcb4d7ea6de322756449c11a0b7545', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat-newbpe-q4_0.gguf', 'filesize': '3912373472', 'requires': '2.7.1', 'removedIn': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf', 'promptTemplate': '<|im_start|>user\\n%1<|im_end|>\\n<|im_start|>assistant\\n%2<|im_end|>\\n', 'systemPrompt': '<|im_start|>system\\n- You are a helpful assistant chatbot trained by MosaicML.\\n- You answer questions.\\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\\n', 'chatTemplate': \"{%- for message in messages %}\\n    {{- '<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>\\\\n' }}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\"}]\n",
            "[{'order': 'p', 'md5sum': 'ab5d8e8a2f79365ea803c1f1d0aa749d', 'name': 'MPT Chat', 'filename': 'mpt-7b-chat.gguf4.Q4_0.gguf', 'filesize': '3796178112', 'requires': '2.7.3', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'MPT', 'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>', 'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf', 'promptTemplate': '<|im_start|>user\\n%1<|im_end|>\\n<|im_start|>assistant\\n%2<|im_end|>\\n', 'systemPrompt': '<|im_start|>system\\n- You are a helpful assistant chatbot trained by MosaicML.\\n- You answer questions.\\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\\n', 'chatTemplate': \"{%- for message in messages %}\\n    {{- '<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>\\\\n' }}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\"}, {'order': 'q', 'md5sum': 'f8347badde9bfc2efbe89124d78ddaf5', 'name': 'Phi-3 Mini Instruct', 'filename': 'Phi-3-mini-4k-instruct.Q4_0.gguf', 'filesize': '2176181568', 'requires': '2.7.1', 'ramrequired': '4', 'parameters': '4 billion', 'quant': 'q4_0', 'type': 'Phi-3', 'description': '<ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href=\"https://opensource.org/license/mit\">MIT</a></li><li>No restrictions on commercial use</li></ul>', 'url': 'https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf', 'promptTemplate': '<|user|>\\n%1<|end|>\\n<|assistant|>\\n%2<|end|>\\n', 'systemPrompt': '', 'chatTemplate': \"{{- bos_token }}\\n{%- for message in messages %}\\n    {{- '<|' + message['role'] + '|>\\\\n' + message['content'] + '<|end|>\\\\n' }}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|assistant|>\\\\n' }}\\n{%- else %}\\n    {{- eos_token }}\\n{%- endif %}\"}, {'order': 'r', 'md5sum': '0e769317b90ac30d6e09486d61fefa26', 'name': 'Mini Orca (Small)', 'filename': 'orca-mini-3b-gguf2-q4_0.gguf', 'filesize': '1979946720', 'requires': '2.5.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'OpenLLaMa', 'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>', 'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf', 'promptTemplate': '### User:\\n%1\\n\\n### Response:\\n', 'systemPrompt': '### System:\\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\\n\\n', 'chatTemplate': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set loop_start = 1 %}\\n    {{- '### System:\\\\n' + messages[0]['content'] + '\\\\n\\\\n' }}\\n{%- else %}\\n    {%- set loop_start = 0 %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if loop.index0 >= loop_start %}\\n        {%- if message['role'] == 'user' %}\\n            {{- '### User:\\\\n' + message['content'] + '\\\\n\\\\n' }}\\n        {%- elif message['role'] == 'assistant' %}\\n            {{- '### Response:\\\\n' + message['content'] + '\\\\n\\\\n' }}\\n        {%- else %}\\n            {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '### Response:\\\\n' }}\\n{%- endif %}\"}, {'order': 's', 'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e', 'disableGUI': 'true', 'name': 'Replit', 'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf', 'filesize': '1953055104', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '3 billion', 'quant': 'q4_0', 'type': 'Replit', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 't', 'md5sum': '70841751ccd95526d3dcfa829e11cd4c', 'disableGUI': 'true', 'name': 'Starcoder', 'filename': 'starcoder-newbpe-q4_0.gguf', 'filesize': '8987411904', 'requires': '2.6.0', 'ramrequired': '4', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Starcoder', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>', 'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf', 'chatTemplate': None}, {'order': 'u', 'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83', 'disableGUI': 'true', 'name': 'Rift coder', 'filename': 'rift-coder-v0-7b-q4_0.gguf', 'filesize': '3825903776', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'LLaMA', 'systemPrompt': '', 'promptTemplate': '%1', 'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>', 'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf', 'chatTemplate': None}, {'order': 'v', 'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7', 'disableGUI': 'true', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2-f16.gguf', 'filesize': '45887744', 'requires': '2.5.0', 'removedIn': '2.7.4', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf', 'chatTemplate': None}, {'order': 'w', 'md5sum': 'dd90e2cb7f8e9316ac3796cece9883b5', 'name': 'SBert', 'filename': 'all-MiniLM-L6-v2.gguf2.f16.gguf', 'filesize': '45949216', 'requires': '2.7.4', 'removedIn': '3.0.0', 'ramrequired': '1', 'parameters': '40 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)', 'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf', 'chatTemplate': None}, {'order': 'x', 'md5sum': '919de4dd6f25351bcb0223790db1932d', 'name': 'EM German Mistral', 'filename': 'em_german_mistral_v01.Q4_0.gguf', 'filesize': '4108916352', 'requires': '2.5.0', 'ramrequired': '8', 'parameters': '7 billion', 'quant': 'q4_0', 'type': 'Mistral', 'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>', 'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf', 'promptTemplate': 'USER: %1 ASSISTANT: ', 'systemPrompt': 'Du bist ein hilfreicher Assistent. ', 'chatTemplate': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set loop_start = 1 %}\\n    {{- messages[0]['content'] }}\\n{%- else %}\\n    {%- set loop_start = 0 %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if loop.index0 >= loop_start %}\\n        {%- if not loop.first %}\\n            {{- ' ' }}\\n        {%- endif %}\\n        {%- if message['role'] == 'user' %}\\n            {{- 'USER: ' + message['content'] }}\\n        {%- elif message['role'] == 'assistant' %}\\n            {{- 'ASSISTANT: ' + message['content'] }}\\n        {%- else %}\\n            {{- raise_exception('After the optional system message, conversation roles must be either user or assistant.') }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {%- if messages %}\\n        {{- ' ' }}\\n    {%- endif %}\\n    {{- 'ASSISTANT:' }}\\n{%- endif %}\", 'systemMessage': 'Du bist ein hilfreicher Assistent.'}, {'order': 'y', 'md5sum': '60ea031126f82db8ddbbfecc668315d2', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1', 'filename': 'nomic-embed-text-v1.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf', 'chatTemplate': None}]\n",
            "[{'order': 'z', 'md5sum': 'a5401e7f7e46ed9fcaed5b60a281d547', 'disableGUI': 'true', 'name': 'Nomic Embed Text v1.5', 'filename': 'nomic-embed-text-v1.5.f16.gguf', 'filesize': '274290560', 'requires': '2.7.4', 'ramrequired': '1', 'parameters': '137 million', 'quant': 'f16', 'type': 'Bert', 'embeddingModel': True, 'systemPrompt': '', 'description': 'nomic-embed-text-v1.5', 'url': 'https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf', 'chatTemplate': None}, {'order': 'zzz', 'md5sum': 'a8c5a783105f87a481543d4ed7d7586d', 'name': 'Qwen2-1.5B-Instruct', 'filename': 'qwen2-1_5b-instruct-q4_0.gguf', 'filesize': '937532800', 'requires': '3.0', 'ramrequired': '3', 'parameters': '1.5 billion', 'quant': 'q4_0', 'type': 'qwen2', 'description': '<ul><li>Very fast responses</li><li>Instruction based model</li><li>Usage of LocalDocs (RAG): Highly recommended</li><li>Supports context length of up to 32768</li><li>Trained and finetuned by Qwen (Alibaba Cloud)</li><li>License: <a href=\"https://www.apache.org/licenses/LICENSE-2.0.html/\">Apache 2.0</a></li></ul>', 'url': 'https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-q4_0.gguf', 'promptTemplate': '<|im_start|>user\\n%1<|im_end|>\\n<|im_start|>assistant\\n%2<|im_end|>', 'systemPrompt': '<|im_start|>system\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\\n', 'chatTemplate': \"{%- for message in messages %}\\n    {%- if loop.first and messages[0]['role'] != 'system' %}\\n        {{- '<|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n    {{- '<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>\\\\n' }}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(model_name=\"gpt4all-lora-quantized.bin\")  # Specify the quantized model here, no such name in gpt4all\n",
        "model.open()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "c0sswh6XIrr2",
        "outputId": "ae28feb6-6f2e-4585-9d0a-e5fae47f1bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Request failed: HTTP 404 Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-370de894dfa6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgpt4all\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT4All\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT4All\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt4all-lora-quantized.bin\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Specify the quantized model here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gpt4all/gpt4all.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# Retrieve model and download if allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConfigType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gpt4all/gpt4all.py\u001b[0m in \u001b[0;36mretrieve_model\u001b[0;34m(cls, model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;31m# If model file does not exist, download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mfilesize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             config[\"path\"] = str(cls.download_model(\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0mmodel_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0mexpected_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilesize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_md5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"md5sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gpt4all/gpt4all.py\u001b[0m in \u001b[0;36mdownload_model\u001b[0;34m(model_filename, model_path, verbose, url, expected_size, expected_md5)\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mtotal_size_in_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content-length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gpt4all/gpt4all.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(offset)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m206\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Request failed: HTTP {response.status_code} {response.reason}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m206\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Content-Range'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Connection was interrupted and server does not support range requests'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Request failed: HTTP 404 Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "\n",
        "\n",
        "# Initialize the model with the correct name\n",
        "m = GPT4All(model_name=\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")  # For general chat\n",
        "# m = GPT4All(model_name=\"qwen2.5-coder-7b-instruct-q4_0.gguf\")  # For coding tasks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "puVtS2YnH_xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(m))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfvzRVl7XJ1u",
        "outputId": "c1d1fba4-13a1-4a77-d2bf-be4318ef9811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_current_prompt_template', '_format_chat_prompt_template', '_history', 'backend', 'chat_session', 'close', 'config', 'current_chat_session', 'device', 'download_model', 'generate', 'list_gpus', 'list_models', 'model', 'model_type', 'retrieve_model']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = m.generate('Please answer this question: What is a Large Language Model?')\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkSY4c3LXgrr",
        "outputId": "975809e3-eb9e-47db-8cd4-f7a5e69ab38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (LLM)\n",
            "A large language model, or LLM for short, refers to an artificial intelligence (AI) system that has been trained on vast amounts of text data and can generate human-like responses. These models are designed to understand the nuances of natural language processing, including syntax, semantics, and pragmatics.\n",
            "An LLM is typically a type of neural network architecture that uses deep learning techniques to analyze large datasets of text. The model learns patterns and relationships within this data by iteratively refining its predictions through backpropagation.\n",
            "\n",
            "The key characteristics of an LLM include:\n",
            "\n",
            "1. **Scale**: Large language models are trained on massive amounts of text, often in the order of tens or hundreds of millions of words.\n",
            "2. **Complexity**: These models typically consist of multiple layers and neurons that process input data to generate output responses.\n",
            "3. **Contextual understanding**: An LLM is designed to comprehend context-dependent relationships within language, allowing it to generate more accurate and relevant responses.\n",
            "\n",
            "Large\n"
          ]
        }
      ]
    }
  ]
}